# -*- coding: utf-8 -*-
"""Pratilipi-Assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BDsMA4-3Yuo6nIRjgG_0a7--ZYR7yPv1

# Recommendation System

At Pratilipi, we call one story as pratilipi. Attached is a data set containing user_id, pratilipi_id, date. This denotes which user has read which story on a particular date. The data set is provided.

1. Can you build a model using the data set and predict which pratilipis (atleast 5), each user is going to read later?

2. Arrange the dataset in ascending order of time, use the first 75% of the data for training and evaluate your model on the next 25% of the data.

## Data gathering
"""

!gdown --id 1UHJDbnbndi2G9dEgVnNSrUSAXa_QDYBo
!unzip -q ds-assignment.zip

"""## Imports"""

import pickle
import numpy as np
import pandas as pd
from typing import Dict, Set, List
from sklearn.metrics.pairwise import cosine_similarity

"""## Read the data"""

user_df = pd.read_csv('./ds-assignment/user-interactions.csv')
user_df.head()

print(user_df.shape)

metadata_df = pd.read_csv('./ds-assignment/metadata.csv')
metadata_df.head()

print(metadata_df.shape)

"""### Check the Detail information of the dataframe"""

user_df.info()

metadata_df.info()

"""### Describe the entire dataset"""

user_df.describe()

metadata_df.describe()

"""## Data Cleaning

### Check `null` values
"""

user_df.isnull().sum()

metadata_df.isnull().sum()

metadata_df = metadata_df.dropna()

metadata_df.isnull().sum()

"""### Check for duplicate rows"""

def get_duplicate_rows(dataFrame):
    duplicate_rows = dataFrame[dataFrame.duplicated()]
    print(duplicate_rows.shape[0])

get_duplicate_rows(user_df)
get_duplicate_rows(metadata_df)

"""## Data EDA

### Getting all the common categories
"""

metadata_df["category_name"].value_counts()

"""### Read percent  """

user_df["read_percent"].value_counts()

"""## Data preprocessing

### Convert `float` type to `int`
"""

user_df['read_percent'] = user_df['read_percent'].apply(lambda x: int(x))

user_df.drop(user_df[user_df["read_percent"] > 100].index, inplace=True) # removing rows which have read_percent > 100
user_df.drop(user_df[user_df["read_percent"] < 100].index, inplace=True) # removing rows which have read_percent < 100

user_df.sort_values('updated_at', ascending=False, inplace=True) # Sort the dataframe according to `updated_at`

"""## Split the data"""

test_split=0.25
test, train = np.split(user_df, [int(test_split * len(user_df))])

test.to_csv('./test.csv', sep=',', encoding='utf-8')
train.to_csv('./train.csv', sep=',', encoding='utf-8')

"""### Read the test and train data"""

train_df = pd.read_csv('./train.csv')
print(train_df.shape)

test_df = pd.read_csv('./test.csv')
print(test_df.shape)

"""## Get unique users and number of the unique users"""

unique_users = set(train['user_id'])
unique_no_users = len(train['user_id'])

"""## Get unique pratilipi and number of the unique pratilipi"""

unique_pratilipi = set(train['pratilipi_id'])
unique_no_pratilipi = len(train['pratilipi_id'])

def create_dict(unique_features: Set, dict_one: Dict, dict_two: Dict):
    index = 0
    for unique_feature in unique_features:
        dict_one[unique_feature] = index
        dict_two[index] = unique_feature
        index += 1
    return dict_one, dict_two

user_to_index = {}
index_to_user = {}

user_to_index, index_to_user = create_dict(unique_users, user_to_index, index_to_user) 

pratilipi_to_index = {}
index_to_pratilipi = {}

pratilipi_to_index, index_to_pratilipi = create_dict(unique_pratilipi, pratilipi_to_index, index_to_pratilipi)

"""## Create pivot table"""

user_to_read_pratilipis = {}
users = train['user_id']
for user in unique_users:
    user_to_read_pratilipis[user] = []

user_len = len(users)
count = 0

for train_index, train_row in train.iterrows():
    key = train_row['user_id']
    val = (train_row['pratilipi_id'], train_row['read_percent'])
    user_to_read_pratilipis[key].append(val)
    count += 1

count = 0
with open('./pivot.pickle', "wb") as pivot_file:
    for user in unique_users:
        csv_row = [-1 for x in range(unique_no_pratilipi)]
        pratilipis_read = user_to_read_pratilipis[user]
        if pratilipis_read is None or len(pratilipis_read) == 0:
            pass
        else:
            for pratilipi_and_read_percent in pratilipis_read:
                col_index = pratilipi_to_index[pratilipi_and_read_percent[0]]
                csv_row[col_index] = pratilipi_and_read_percent[1]
        pickle.dump(np.array(csv_row), pivot_file)
        count += 1

"""## Create Similarity Matrix"""

def get_row_from_pickle_file(filename: str, row_index: int):
    with open(filename, "rb") as pivot_file:
        for cur_index in range(row_index+1):
            row = pickle.load(pivot_file)
            if cur_index == row_index:
                return row
            else:
                pass
    return None

count = 0
with open('./similarity.pickle', "wb") as similarity_file:
    for cur_index in range(unique_no_users):

        cur_feature = get_row_from_pickle_file(pivot_filename, cur_index)

        with open('./pivot.pickle', "rb") as pivot_file:
            cur_similarity = []
            index = 0
            while True:
                chunk = []
                for chunk_index in range(100):
                    if index == unique_no_users:
                        break
                    else:
                        feature = pickle.load(pivot_file)
                        chunk.append(feature)
                        index += 1

                if len(chunk) != 0:
                    chunk = np.array(chunk)
                    mat = np.vstack((cur_feature, chunk))
                    similarities = cosine_similarity(mat)

                    for similarity in similarities[0][1:]:
                        cur_similarity.append(similarity)
                else:
                    break

            cur_similarity = np.array(cur_similarity)

            pickle.dump(cur_similarity, similarity_file)
            count += 1

"""## Store Recommendation"""

with open('./recommendation.pickle', "wb") as recommendation_file:
    with open('./similarity.pickle', "rb") as similarity_file:
        for cur_user_index in range(unique_no_users):
            cur_user_similarity = pickle.load(similarity_file)

            cur_user_sorted_indexes = cur_user_similarity.argsort()[::-1]

            top_similar_users_index = cur_user_sorted_indexes[:5+1]

            temp_index = np.argwhere(top_similar_users_index == cur_user_index)
            top_similar_users_index = np.delete(top_similar_users_index, temp_index)

            top_similar_user_similarity = cur_user_similarity[top_similar_users_index]

            cur_user_watched_movies_index = []
            cur_user_scores = get_row_from_pickle_file(pivot_filename, cur_user_index)
            for index in range(unique_no_pratilipis):
                if cur_user_scores[index] != -1:
                    cur_user_watched_movies_index.append(index)

            top_score_index_list = []
            weight_list = []

            for similar_user_index in top_similar_users_index:
                similar_user_scores = get_row_from_pickle_file(pivot_filename, similar_user_index)

                similar_user_sorted_scores_index = similar_user_scores.argsort()[::-1]

                similar_user_top_scores_index = []
                for index in similar_user_sorted_scores_index:
                    if index not in cur_user_watched_movies_index:
                        similar_user_top_scores_index.append(index)
                        if len(similar_user_top_scores_index) == 5:
                            break

                similar_user_top_scores_index = np.array(similar_user_top_scores_index)
                similar_user_top_scores = similar_user_scores[similar_user_top_scores_index]

                similar_user_weights = similar_user_top_scores * cur_user_similarity[similar_user_index]

                for index in similar_user_top_scores_index:
                    top_score_index_list.append(index)

                for w in similar_user_weights:
                    weight_list.append(w)

            weight_list = np.array(weight_list)
            top_weight_index = weight_list.argsort()[::-1][:5]

            top_pratilipis = [index_to_pratilipi[x] for x in top_weight_index]

            pickle.dump(top_weight_index, recommendation_file)

"""## Evaluate the model"""

def calculate_accuracy(user_name_list: List, recommended_movies_list: List):
    test = pd.read_csv(test_filename)

    accuracy_metrics_list = []
    for x in range(len(user_name_list)):
        accuracy_metrics_obj = accuracy_metrics()
        accuracy_metrics_list.append(accuracy_metrics_obj)

    for index, row in test.iterrows():
        match_index = None
        try:
            match_index = user_name_list.index(row["user_id"])
        except ValueError as e:
            pass
        if match_index is not None:
            accuracy_metrics_list[match_index].total_test_movies_watched += 1
            if row["pratilipi_id"] in recommended_movies_list[match_index]:
                accuracy_metrics_list[match_index].cover_count += 1
                accuracy_metrics_list[match_index].weighted_cover += float(row["read_percentage"])/100.0
            else:
                pass
        else:
            pass

    for accuracy_metrics_obj in accuracy_metrics_list:
        if accuracy_metrics_obj.total_test_movies_watched > accuracy_metrics_obj.num_recommend_movies:
            accuracy_metrics_obj.cover_percentage = (accuracy_metrics_obj.cover_count*1.0/accuracy_metrics_obj.num_recommend_movies)*100
            accuracy_metrics_obj.weighted_cover_percentage = (accuracy_metrics_obj.weighted_cover*1.0/accuracy_metrics_obj.num_recommend_movies)*100
        else:
            accuracy_metrics_obj.cover_percentage = (accuracy_metrics_obj.cover_count*1.0/accuracy_metrics_obj.total_test_movies_watched)*100
            accuracy_metrics_obj.weighted_cover_percentage = (accuracy_metrics_obj.weighted_cover*1.0/accuracy_metrics_obj.total_test_movies_watched)*100

    return accuracy_metrics_list

test_unique_users = list(set(test_df["user_id"]))
recomended_pratilipis = get_recommendation(test_unique_users)

accuracy_metrics_list = calculate_accuracy(test_unique_users, recomended_pratilipis)

average_cover_percentage = 0
weighted_cover_percentage = 0
for accuracy_metrics_obj in accuracy_metrics_list:
    average_cover_percentage += accuracy_metrics_obj.cover_percentage
    weighted_cover_percentage += accuracy_metrics_obj.weighted_cover_percentage

print(f'Average Cover Percentage: {average_cover_percentage}')
print(f'Weighted Cover Percentage: {weighted_cover_percentage}')